{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujwNDt2WsOHq",
        "outputId": "630ceeaa-c1a8-47ba-c88e-6af6a8e809e9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "\n",
        "RESERVED_WORDS_LIST = [\n",
        "    \"TextWindow\",\n",
        "    \"ElseIf\",\n",
        "    \"EndIf\",\n",
        "    \"EndWhile\",\n",
        "    \"EndFor\",\n",
        "    \"EndSub\",\n",
        "    \"Goto\",\n",
        "    \"If\",\n",
        "    \"Then\",\n",
        "    \"Else\",\n",
        "    \"While\",\n",
        "    \"For\",\n",
        "    \"Sub\",\n",
        "    \"And\",\n",
        "    \"Or\",\n",
        "    \"Array\",\n",
        "    \"To\",\n",
        "    \"Step\",\n",
        "    \"Stack\",\n",
        "    \"Program\",\n",
        "]\n",
        "\n",
        "\n",
        "SPECIAL_SYMBOLS_DICT = {\n",
        "    \"<>\": \"tkn_diff\",\n",
        "    \"<=\": \"tkn_leq\",\n",
        "    \">=\": \"tkn_geq\",\n",
        "}\n",
        "\n",
        "SYMBOLS_DICT = {\n",
        "    \"=\": \"tkn_equals\",\n",
        "    \".\": \"tkn_period\",\n",
        "    \",\": \"tkn_comma\",\n",
        "    \":\": \"tkn_colon\",\n",
        "    \"[\": \"tkn_left_brac\",\n",
        "    \"]\": \"tkn_right_brac\",\n",
        "    \"(\": \"tkn_left_paren\",\n",
        "    \")\": \"tkn_right_paren\",\n",
        "    \"+\": \"tkn_plus\",\n",
        "    \"-\": \"tkn_minus\",\n",
        "    \"*\": \"tkn_times\",\n",
        "    \"/\": \"tkn_div\",\n",
        "    \"<\": \"tkn_less\",\n",
        "    \">\": \"tkn_greater\",\n",
        "}\n",
        "\n",
        "\n",
        "NUMBER_REGEX_PATTERN = r\"^\\d+\\.?\\d*$\"\n",
        "\n",
        "BOOLEANS_REGEX_PATTERN = r'\"(true|false)\"'\n",
        "\n",
        "TXT_REGEX_PATTERN = r'\".*?\"'\n",
        "\n",
        "SPECIAL_SYMBOLS_REGEX_PATTERN = r\"(?:<=|>=|<>)\"\n",
        "\n",
        "SYMBOLS_REGEX_PATTERN = \"|\".join([re.escape(symbol[0]) for symbol in SYMBOLS_DICT])\n",
        "SYMBOLS_REGEX_PATTERN = r\"[\" + SYMBOLS_REGEX_PATTERN + r\"]\"\n",
        "\n",
        "ID_REGEX_PATTERN = r\"^[^\\W\\d_]\\w*$\"\n",
        "\n",
        "RESERVED_WORDS_REGEX_PATTERN = r\"\\b(?:\" + \"|\".join(RESERVED_WORDS_LIST) + r\")\\b\"\n",
        "\n",
        "TOKEN_LIST = [\n",
        "    (\"num\", re.compile(NUMBER_REGEX_PATTERN)),\n",
        "    (\"boolean\", re.compile(BOOLEANS_REGEX_PATTERN, re.IGNORECASE)),\n",
        "    (\"str\", re.compile(TXT_REGEX_PATTERN)),\n",
        "    (\"special_symbol\", re.compile(SPECIAL_SYMBOLS_REGEX_PATTERN)),\n",
        "    (\"symbol\", re.compile(SYMBOLS_REGEX_PATTERN)),\n",
        "    (\"reserved_word\", re.compile(RESERVED_WORDS_REGEX_PATTERN)),\n",
        "    (\"id\", re.compile(ID_REGEX_PATTERN)),\n",
        "]\n",
        "\n",
        "\n",
        "def classify_token(token, lex):\n",
        "    if token == \"special_symbol\":\n",
        "        return lex\n",
        "    if token == \"symbol\":\n",
        "        return lex\n",
        "    if token == \"reserved_word\":\n",
        "        return lex\n",
        "    if token == \"boolean\":\n",
        "        new_string = lex[1:-1]\n",
        "        return new_string.capitalize()\n",
        "    return token\n",
        "\n",
        "\n",
        "def aggregate_lex(token, lex):\n",
        "    if token == \"str\":\n",
        "        new_string = lex[1:-1]\n",
        "        return new_string\n",
        "    return lex\n",
        "\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, token, lex, row, column):\n",
        "        self.token = classify_token(token, lex)\n",
        "        self.lex = aggregate_lex(token, lex)\n",
        "        self.row = row\n",
        "        self.column = column\n",
        "        self.token_type = token\n",
        "\n",
        "    def __str__(self):\n",
        "        special_cases = [\"special_symbol\", \"symbol\", \"reserved_word\", \"boolean\", \"$\"]\n",
        "        if self.token_type in special_cases:\n",
        "            return \"<{}, {}, {}>\".format(self.token, self.row, self.column)\n",
        "        return \"<{}, {}, {}, {}>\".format(self.token, self.lex, self.row, self.column)\n",
        "\n",
        "\n",
        "def lexical(user_input):\n",
        "    tokens = []\n",
        "    lines = user_input.split(\"\\n\")\n",
        "    abort_analysis = False\n",
        "    for i in range(len(lines)):\n",
        "        row = lines[i]\n",
        "        j = 0\n",
        "\n",
        "        while j < len(row):\n",
        "            match = None\n",
        "            # Ignore spaces\n",
        "            if row[j] == \" \":\n",
        "                j += 1\n",
        "                continue\n",
        "\n",
        "            # Jump line if a comment is found\n",
        "            if row[j] == \"'\":\n",
        "                break\n",
        "\n",
        "            line_end = len(row)\n",
        "            break_loop = False\n",
        "            while j <= line_end:\n",
        "                word = row[j:line_end]\n",
        "\n",
        "                for token_type, compiled_regex in TOKEN_LIST:\n",
        "                    match = compiled_regex.match(word)\n",
        "                    if match:\n",
        "                        # Assign values\n",
        "                        token_value = match.group()\n",
        "                        token_start = match.start()\n",
        "                        token_end = match.end()\n",
        "                        current_token = Token(token_type, token_value, i + 1, j + 1)\n",
        "\n",
        "                        # Reassign j\n",
        "                        j += token_end\n",
        "\n",
        "                        # Print the value\n",
        "                        tokens.append(current_token)\n",
        "\n",
        "                        break_loop = True\n",
        "                        break\n",
        "\n",
        "                if break_loop:\n",
        "                    break\n",
        "                line_end -= 1\n",
        "\n",
        "            if not match:\n",
        "                error = Token(\n",
        "                    \"lexical_error\",\n",
        "                    \">>> Lexical Error (Line: {}, Pos: {})\".format(i + 1, j + 1),\n",
        "                    i + 1,\n",
        "                    j + 1,\n",
        "                )\n",
        "                tokens.append(error)\n",
        "                abort_analysis = True\n",
        "                break\n",
        "        # endfor\n",
        "        if abort_analysis:\n",
        "            break\n",
        "    if not abort_analysis:\n",
        "        tokens.append(Token(\"$\", \"$\", i + 1, j + 1))\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def get_productions_first_set(grammar, productions):\n",
        "    prod_first_set = set()\n",
        "    if productions == [\"eps\"]:\n",
        "        prod_first_set.add(\"eps\")\n",
        "        return prod_first_set\n",
        "\n",
        "    if productions[0] not in grammar:\n",
        "        prod_first_set.add(productions[0])\n",
        "        return prod_first_set\n",
        "\n",
        "    first_a1 = get_rule_first_set(grammar, grammar[productions[0]])\n",
        "    if \"eps\" in first_a1:\n",
        "        if len(productions) > 1:\n",
        "            first_a1.discard(\"eps\")\n",
        "            first_a2_an = get_productions_first_set(grammar, productions[1:])\n",
        "            prod_first_set.update(first_a1)\n",
        "            prod_first_set.update(first_a2_an)\n",
        "            return prod_first_set\n",
        "    return first_a1\n",
        "\n",
        "\n",
        "def get_rule_first_set(grammar, rule):\n",
        "    rule_first_set = set()\n",
        "    for subrule in rule:\n",
        "        prod_first_set = get_productions_first_set(grammar, subrule)\n",
        "        rule_first_set.update(prod_first_set)\n",
        "    return rule_first_set\n",
        "\n",
        "\n",
        "def get_grammar_first(grammar):\n",
        "    grammar_first = {}\n",
        "    for rule in grammar:\n",
        "        grammar_first[rule] = get_rule_first_set(grammar, grammar[rule])\n",
        "    return grammar_first\n",
        "\n",
        "\n",
        "def get_last_occurence_index(item, li):\n",
        "    return len(li) - 1 - li[::-1].index(item)\n",
        "\n",
        "\n",
        "def get_grammar_rule_occurrences(grammar):\n",
        "    # Check for appearences in the grammar\n",
        "    def search_in_grammar(key):\n",
        "        matches = []\n",
        "        for rule, productions in grammar.items():\n",
        "            for production in productions:\n",
        "                if key in production:\n",
        "                    matches.append((rule, production))\n",
        "        return matches\n",
        "\n",
        "    grammar_rule_occurrences = {}\n",
        "    # For every key fetch for appearences in all grammar\n",
        "    for rule_key in grammar:\n",
        "        grammar_rule_occurrences[rule_key] = search_in_grammar(rule_key)\n",
        "    return grammar_rule_occurrences\n",
        "\n",
        "\n",
        "def get_grammar_next_set(grammar, initial_rule_key):\n",
        "    grammar_first = get_grammar_first(grammar)\n",
        "    grammar_rule_occurrence = get_grammar_rule_occurrences(grammar)\n",
        "\n",
        "    calculated_next = {}\n",
        "\n",
        "    def get_next_set(rule_key):\n",
        "        # If it already exists, then return it\n",
        "        if rule_key in calculated_next:\n",
        "            return calculated_next[rule_key]\n",
        "\n",
        "        # Get places where it appears\n",
        "        rule_occurrences = grammar_rule_occurrence[rule_key]\n",
        "\n",
        "        # Create a new set\n",
        "        next_set = set()\n",
        "\n",
        "        # If it is the initial key, then add $\n",
        "        if rule_key == initial_rule_key:\n",
        "            next_set.add(\"$\")\n",
        "\n",
        "        # Now go for each occurrence\n",
        "        for occ_key, occ_production in rule_occurrences:\n",
        "            key_index = get_last_occurence_index(rule_key, occ_production)\n",
        "            if key_index + 1 == len(occ_production):  # Is the last one\n",
        "                if occ_key != rule_key:\n",
        "                    s_A = get_next_set(occ_key)\n",
        "                    next_set.update(s_A)\n",
        "            else:\n",
        "                beta = occ_production[key_index + 1]\n",
        "                if beta not in grammar:\n",
        "                    next_set.add(beta)\n",
        "                else:\n",
        "                    f_beta = grammar_first[beta]\n",
        "                    if \"eps\" in f_beta:\n",
        "                        f_beta.discard(\"eps\")\n",
        "                        next_set.update(f_beta)\n",
        "                        s_A = get_next_set(occ_key) if occ_key != rule_key else set()\n",
        "                        next_set.update(s_A)\n",
        "                    else:\n",
        "                        next_set.update(f_beta)\n",
        "\n",
        "        # At the end, return the set obviously,\n",
        "        # But first, add it to the calculated ones\n",
        "        calculated_next[rule_key] = next_set\n",
        "        return next_set\n",
        "\n",
        "    grammar_next = {}\n",
        "    for key in grammar:\n",
        "        key_next_set = get_next_set(key)\n",
        "        grammar_next[key] = key_next_set\n",
        "\n",
        "    return grammar_next\n",
        "\n",
        "\n",
        "\n",
        "def get_grammar_pred_set(grammar, initial_rule_key):\n",
        "    grammar_next = get_grammar_next_set(grammar, initial_rule_key)\n",
        "\n",
        "    def get_pred_set(A, a):\n",
        "        pred_set = set()\n",
        "        f_a = get_productions_first_set(grammar, a)\n",
        "        if \"eps\" in f_a:\n",
        "            f_a.discard(\"eps\")\n",
        "            pred_set.update(f_a)\n",
        "\n",
        "            s_A = grammar_next[A]\n",
        "            pred_set.update(s_A)\n",
        "        else:\n",
        "            pred_set.update(f_a)\n",
        "        return pred_set\n",
        "\n",
        "    pred_set_dict = {}\n",
        "    for rule, productions in grammar.items():\n",
        "        pred_set_dict[rule] = []\n",
        "        for prod in productions:\n",
        "            pred_set = get_pred_set(rule, prod)\n",
        "            pred_set_dict[rule].append(pred_set)\n",
        "    return pred_set_dict\n",
        "\n",
        "\n",
        "grammar = {\n",
        "    \"P\": [[\"eps\"], [\"SU\", \"P\"], [\"S\", \"P\"]],\n",
        "    \"S\": [[\"C\"], [\"L\"], [\"BF\"], [\"ID\"], [\"GT\"]],\n",
        "    \"ID\": [[\"id\", \"ID1\"]],\n",
        "    \"ID1\": [[\"eps\"], [\"VA\"], [\":\"], [\"(\", \")\"], [\"A\"]],\n",
        "    \"VA\": [[\"VA_OP_DIM\", \"=\", \"EX\"]],\n",
        "    \"VA_OP_DIM\": [[\"eps\"], [\"[\", \"EX\", \"]\", \"VA_OP_DIM\"]],\n",
        "    \"C\": [[\"If\", \"(\", \"EX\", \")\", \"Then\", \"CSL\", \"C1\"]],\n",
        "    \"C1\": [\n",
        "        [\"EndIf\"],\n",
        "        [\"ElseIf\", \"(\", \"EX\", \")\", \"Then\", \"CSL\", \"C1\"],\n",
        "        [\"Else\", \"CSL\", \"EndIf\"],\n",
        "    ],\n",
        "    \"CSL\": [[\"eps\"], [\"S\", \"CSL\"]],\n",
        "    \"L\": [[\"F\"], [\"W\"]],\n",
        "    \"F\": [[\"For\", \"id\", \"VA\", \"To\", \"EX\", \"ST\", \"FSL\", \"EndFor\"]],\n",
        "    \"ST\": [[\"eps\"], [\"Step\", \"ST1\"]],\n",
        "    \"ST1\": [[\"-\", \"ES\"], [\"ES\"]],\n",
        "    \"FSL\": [[\"eps\"], [\"S\", \"FSL\"]],\n",
        "    \"W\": [[\"While\", \"(\", \"EX\", \")\", \"WSL\", \"EndWhile\"]],\n",
        "    \"WSL\": [[\"eps\"], [\"S\", \"WSL\"]],\n",
        "    \"GT\": [[\"Goto\", \"id\"]],\n",
        "    \"SU\": [[\"Sub\", \"id\", \"SUSL\", \"EndSub\"]],\n",
        "    \"SUSL\": [[\"eps\"], [\"S\", \"SUSL\"]],\n",
        "    \"A\": [[\"DIL\", \"=\", \"EX\"]],\n",
        "    \"DIL\": [[\"[\", \"EX\", \"]\", \"A_OP_DIM\"]],\n",
        "    \"A_OP_DIM\": [[\"eps\"], [\"[\", \"EX\", \"]\", \"A_OP_DIM\"]],\n",
        "    \"BF\": [[\"RW\", \".\", \"id\", \"(\", \"PAM\", \")\"]],\n",
        "    \"PAM\": [[\"eps\"], [\"PAL\"]],\n",
        "    \"PAL\": [[\"PA\", \"PAL1\"]],\n",
        "    \"PAL1\": [[\"eps\"], [\",\", \"PA\", \"PAL1\"]],\n",
        "    \"PA\": [[\"EX\"], [\"eps\"]],\n",
        "    \"RW\": [[\"TextWindow\"], [\"Stack\"], [\"Array\"], [\"Program\"]],\n",
        "    \"EX\": [\n",
        "        [\"-\", \"EB\", \"EX1\"],\n",
        "        [\"EB\", \"EX1\"],\n",
        "    ],\n",
        "    \"EX1\": [[\"Or\", \"EB\", \"EX1\"], [\"eps\"]],\n",
        "    \"EB\": [[\"ER\", \"EB1\"]],\n",
        "    \"EB1\": [[\"And\", \"ER\", \"EB1\"], [\"eps\"]],\n",
        "    \"ER\": [[\"ES\", \"ER1\"]],\n",
        "    \"ER1\": [[\"RO\", \"ES\"], [\"eps\"]],\n",
        "    \"ES\": [[\"EM\", \"ES1\"]],\n",
        "    \"ES1\": [[\"SO\", \"EM\", \"ES1\"], [\"eps\"]],\n",
        "    \"EM\": [[\"D\", \"EM1\"]],\n",
        "    \"EM1\": [[\"MO\", \"D\", \"EM1\"], [\"eps\"]],\n",
        "    \"SO\": [[\"+\"], [\"-\"]],\n",
        "    \"MO\": [[\"*\"], [\"/\"]],\n",
        "    \"RO\": [[\"<\"], [\">\"], [\"=\"], [\"<=\"], [\">=\"], [\"<>\"]],\n",
        "    \"D\": [\n",
        "        [\"str\"],\n",
        "        [\"True\"],\n",
        "        [\"False\"],\n",
        "        [\"num\"],\n",
        "        [\"id\", \"ID_OP_DIM\"],\n",
        "        [\"(\", \"EX\", \")\"],\n",
        "        [\"BF\"],\n",
        "    ],\n",
        "    \"ID_OP_DIM\": [[\"eps\"], [\"[\", \"EX\", \"]\", \"ID_OP_DIM\"]],\n",
        "}\n",
        "\n",
        "\n",
        "## PARSER\n",
        "def format_expected(expected_tokens):\n",
        "    formatted_tokens = []\n",
        "    for token in expected_tokens:\n",
        "        if token == \"id\":\n",
        "            formatted_tokens.append(\"Identifier\")\n",
        "        elif token == \"num\":\n",
        "            formatted_tokens.append(\"Number\")\n",
        "        elif token == \"str\":\n",
        "            formatted_tokens.append(\"Text\")\n",
        "        elif token == \"True\":\n",
        "            formatted_tokens.append(\"True\")\n",
        "        elif token == \"False\":\n",
        "            formatted_tokens.append(\"False\")\n",
        "        elif token == \"$\":\n",
        "            formatted_tokens.append(\"EOF\")\n",
        "        else:\n",
        "            formatted_tokens.append(token)\n",
        "    formatted_tokens.sort()\n",
        "    return formatted_tokens\n",
        "\n",
        "\n",
        "def parse(user_input, grammar, initial_symbol):\n",
        "    def raise_error(row, column, found, expected_tokens):\n",
        "        formatted = format_expected(expected_tokens)\n",
        "        expected = \", \".join(f\"'{token}'\" for token in formatted)\n",
        "        if found.token == \"$\":\n",
        "            print(f\"[{row}:{column}] Syntax Error: EOF found, expected: {expected}.\")\n",
        "        else:\n",
        "            print(\n",
        "                f\"[{row}:{column}] Syntax Error: Found: '{found.lex}'; expected: {expected}.\"\n",
        "            )\n",
        "\n",
        "    tokens = lexical(user_input)\n",
        "    grammar_pred_sets = get_grammar_pred_set(grammar, initial_symbol)\n",
        "\n",
        "    derivation = [initial_symbol]\n",
        "    syntactic_error = False\n",
        "    while len(derivation) > 0:\n",
        "        a1 = derivation.pop(0)\n",
        "        current_token = tokens[0]\n",
        "        if a1 not in grammar:  # Not in grammar, pair...\n",
        "            if a1 == \"eps\":\n",
        "                if current_token.token == \"$\" and len(derivation) == 0:\n",
        "                    tokens.pop(0)\n",
        "                continue\n",
        "            else:\n",
        "                terminalMatch = current_token.token == a1\n",
        "                if terminalMatch:\n",
        "                    tokens.pop(0)\n",
        "                else:\n",
        "                    syntactic_error = True\n",
        "                    raise_error(\n",
        "                        current_token.row, current_token.column, current_token, [a1]\n",
        "                    )\n",
        "                    break\n",
        "        else:  # It is a non-terminal, lets derivate...\n",
        "            rule_pred_sets = grammar_pred_sets[a1]\n",
        "            for index, pred_set in enumerate(rule_pred_sets):\n",
        "                matched = False\n",
        "                if (\n",
        "                    current_token.token in pred_set\n",
        "                ):  # With the selected one, then replace\n",
        "                    matched = True\n",
        "                    for prod in reversed(grammar[a1][index]):\n",
        "                        derivation.insert(0, prod)\n",
        "                    break\n",
        "\n",
        "            if not matched:\n",
        "                syntactic_error = True\n",
        "                f_a1 = get_productions_first_set(grammar, [a1])\n",
        "                f_a2n = {}\n",
        "                if len(derivation) > 0:\n",
        "                    f_a2n = get_productions_first_set(grammar, [*derivation])\n",
        "                combined_preds = set().union(*rule_pred_sets)\n",
        "                expected = set()\n",
        "                if \"eps\" in f_a1:\n",
        "                    if \"$\" in combined_preds:\n",
        "                        next_is_terminal = False\n",
        "                        for derivation_i in derivation:\n",
        "                            if derivation_i not in grammar:\n",
        "                                next_is_terminal = True\n",
        "                                break\n",
        "                        if not next_is_terminal:\n",
        "                            expected.add(\"$\")\n",
        "                    expected.update(f_a1)\n",
        "                    expected.update(f_a2n)\n",
        "                    expected.discard(\"eps\")\n",
        "                else:\n",
        "                    expected = combined_preds\n",
        "                the_expected = list(expected)\n",
        "                raise_error(\n",
        "                    current_token.row, current_token.column, current_token, the_expected\n",
        "                )\n",
        "                break\n",
        "\n",
        "    if not syntactic_error:\n",
        "        print(\"Syntax analysis finished succesfully.\")\n",
        "\n",
        "\n",
        "test_code = sys.stdin.read()\n",
        "\n",
        "parse(test_code, grammar, \"P\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
